#!/usr/bin/env python3
"""
ç•ªèŒ„å°è¯´æ’è¡Œæ¦œçˆ¬è™« - å‘½ä»¤è¡Œå…¥å£

ç”¨æ³•:
    python main.py scrape                           # æŠ“å–é»˜è®¤åˆ†ç±»å¹¶åœ¨æ§åˆ¶å°å±•ç¤º
    python main.py scrape --gender male             # åªæŠ“å–ç”·é¢‘
    python main.py scrape --period read             # åªæŠ“å–é˜…è¯»æ¦œ
    python main.py scrape --category éƒ½å¸‚æ—¥å¸¸,ç„å¹»    # æŒ‡å®šåˆ†ç±»
    python main.py scrape --export feishu           # æ¨é€åˆ°é£ä¹¦
    python main.py scrape --sort category           # æŒ‰åˆ†ç±»æ’åº
    python main.py scrape --group category          # æŒ‰åˆ†ç±»åˆ†ç»„å±•ç¤º
    python main.py download 7143038691944959011     # ä¸‹è½½æŒ‡å®šå°è¯´
    python main.py download 7143038691944959011 --info-only   # åªæŸ¥çœ‹ä¿¡æ¯
    python main.py categories                       # åˆ—å‡ºæ‰€æœ‰å¯ç”¨åˆ†ç±»
    python main.py feishu-fields                    # æ˜¾ç¤ºé£ä¹¦è¡¨æ ¼æ‰€éœ€å­—æ®µ
"""

import argparse
import sys
import os

import yaml

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers.fanqie import FanqieScraper
from exporters.console import ConsoleExporter
from exporters.feishu import FeishuExporter
from sorter import apply_sort, filter_by_gender, filter_by_category, filter_by_period
from downloader import FanqieDownloader


def _deep_merge(base: dict, override: dict) -> dict:
    """æ·±åº¦åˆå¹¶ä¸¤ä¸ªå­—å…¸ï¼Œoverride ä¸­çš„å€¼è¦†ç›– base"""
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge(result[key], value)
        else:
            result[key] = value
    return result


def load_config() -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒ config.local.yaml è¦†ç›–æ•æ„Ÿä¿¡æ¯"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(base_dir, "config.yaml")
    local_path = os.path.join(base_dir, "config.local.yaml")

    config = {}
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}

    # åˆå¹¶æœ¬åœ°é…ç½®ï¼ˆæ•æ„Ÿä¿¡æ¯ï¼‰ï¼Œlocal è¦†ç›– base
    if os.path.exists(local_path):
        with open(local_path, "r", encoding="utf-8") as f:
            local_config = yaml.safe_load(f) or {}
        config = _deep_merge(config, local_config)

    return config


def get_scraper(source: str, config: dict):
    """æ ¹æ®æ¥æºè·å–çˆ¬è™«å®ä¾‹"""
    scraper_map = {
        "fanqie": FanqieScraper,
        # åç»­åœ¨æ­¤æ·»åŠ æ›´å¤šçˆ¬è™«
    }

    scraper_cls = scraper_map.get(source)
    if not scraper_cls:
        print(f"âŒ ä¸æ”¯æŒçš„æ¥æº: {source}")
        print(f"   æ”¯æŒçš„æ¥æº: {', '.join(scraper_map.keys())}")
        sys.exit(1)

    return scraper_cls(config.get("scrape", {}))


def cmd_scrape(args, config):
    """æ‰§è¡ŒæŠ“å–å‘½ä»¤"""
    source = args.source or config.get("scrape", {}).get("default_source", "fanqie")
    scraper = get_scraper(source, config)

    print(f"ğŸ•· å¼€å§‹æŠ“å– [{scraper.SOURCE_NAME}] æ’è¡Œæ¦œ...")
    print()

    # æŠ“å–æ•°æ®
    if args.category:
        # æŒ‡å®šåˆ†ç±»
        category_names = [c.strip() for c in args.category.split(",")]
        novels = scraper.scrape_categories(
            category_names,
            gender=args.gender,
            period=args.period
        )
    else:
        novels = scraper.scrape_all(
            gender=args.gender,
            period=args.period
        )

    if not novels:
        print("âš  æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
        return

    print(f"\nâœ… å…±æŠ“å–åˆ° {len(novels)} æ¡æ•°æ®")

    # æ’åº
    if args.sort:
        novels = apply_sort(novels, args.sort)

    # æ§åˆ¶å°è¾“å‡º
    console_exporter = ConsoleExporter()
    console_exporter.export(novels, group_by=args.group or "none")

    # é£ä¹¦æ¨é€
    if args.export == "feishu":
        feishu_config = config.get("feishu", {})
        feishu = FeishuExporter(feishu_config)
        feishu.export(novels, clear_existing=not args.append)


def cmd_categories(args, config):
    """åˆ—å‡ºå¯ç”¨åˆ†ç±»"""
    source = args.source or config.get("scrape", {}).get("default_source", "fanqie")
    scraper = get_scraper(source, config)

    categories = scraper.get_categories()

    from rich.console import Console
    from rich.table import Table

    console = Console()
    table = Table(title=f"ğŸ“‹ {scraper.SOURCE_NAME} å¯ç”¨åˆ†ç±»", show_lines=True)
    table.add_column("åˆ†ç±»å", style="bold white")
    table.add_column("é¢‘é“", style="cyan")
    table.add_column("åˆ†ç±»ID", style="dim")

    for cat in categories:
        table.add_row(cat["name"], cat["gender_name"], cat["id"])

    console.print(table)


def cmd_feishu_fields(args, config):
    """æ˜¾ç¤ºé£ä¹¦è¡¨æ ¼æ‰€éœ€å­—æ®µ"""
    feishu_config = config.get("feishu", {})
    feishu = FeishuExporter(feishu_config)
    feishu.create_table_if_needed()


def cmd_download(args, config):
    """ä¸‹è½½å°è¯´"""
    dl_config = config.get("download", {})
    dl = FanqieDownloader(dl_config)

    if args.info_only:
        info = dl.get_book_info(args.book_id)
        if info:
            print(f"ä¹¦å: {info.title}")
            print(f"ä½œè€…: {info.author}")
            print(f"ç®€ä»‹: {info.description[:200]}")
            print(f"æ ‡ç­¾: {', '.join(info.tags)}")
            print(f"ç« èŠ‚æ•°: {info.chapter_count}")
            print(f"å®Œç»“: {info.finished}")
        else:
            print("âŒ è·å–ä¹¦ç±ä¿¡æ¯å¤±è´¥")
    elif args.chapters_only:
        chapters = dl.get_chapter_list(args.book_id)
        print(f"å…± {len(chapters)} ç« :")
        for i, ch in enumerate(chapters[:30], 1):
            vol = f" [{ch.volume}]" if ch.volume else ""
            print(f"  {i}. {ch.title}{vol}")
        if len(chapters) > 30:
            print(f"  ... (è¿˜æœ‰ {len(chapters) - 30} ç« )")
    else:
        result = dl.download_book(args.book_id)
        if result:
            print(f"\nâœ… ä¸‹è½½å®Œæˆ: {result}")
        else:
            print("\nâŒ ä¸‹è½½å¤±è´¥")


def main():
    parser = argparse.ArgumentParser(
        description="ğŸ“š å°è¯´æ’è¡Œæ¦œçˆ¬è™« - æŠ“å–ã€æ’åºã€æ¨é€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py scrape                             æŠ“å–æ‰€æœ‰æ’è¡Œæ¦œ
  python main.py scrape --gender male --period read æŠ“å–ç”·é¢‘é˜…è¯»æ¦œ
  python main.py scrape --category éƒ½å¸‚æ—¥å¸¸          æŒ‡å®šåˆ†ç±»
  python main.py scrape --export feishu             æ¨é€åˆ°é£ä¹¦
  python main.py scrape --sort category --group category  æŒ‰åˆ†ç±»æ’åºå’Œåˆ†ç»„
  python main.py categories                         åˆ—å‡ºæ‰€æœ‰åˆ†ç±»
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # scrape å‘½ä»¤
    scrape_parser = subparsers.add_parser("scrape", help="æŠ“å–æ’è¡Œæ¦œæ•°æ®")
    scrape_parser.add_argument(
        "--source", type=str, default=None,
        help="æ•°æ®æ¥æº (é»˜è®¤: fanqie)"
    )
    scrape_parser.add_argument(
        "--gender", type=str, choices=["male", "female"], default=None,
        help="é¢‘é“ç­›é€‰: male(ç”·é¢‘) / female(å¥³é¢‘)"
    )
    scrape_parser.add_argument(
        "--period", type=str, choices=["read", "new"], default=None,
        help="æ¦œå•ç±»å‹: read(é˜…è¯»æ¦œ) / new(æ–°ä¹¦æ¦œ)"
    )
    scrape_parser.add_argument(
        "--category", type=str, default=None,
        help="åˆ†ç±»åç§°ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš” (å¦‚: éƒ½å¸‚æ—¥å¸¸,ç„å¹»)"
    )
    scrape_parser.add_argument(
        "--sort", type=str, choices=["rank", "category", "gender", "period"],
        default=None, help="æ’åºæ–¹å¼"
    )
    scrape_parser.add_argument(
        "--group", type=str, choices=["none", "category", "gender"],
        default=None, help="åˆ†ç»„å±•ç¤ºæ–¹å¼"
    )
    scrape_parser.add_argument(
        "--export", type=str, choices=["feishu"], default=None,
        help="å¯¼å‡ºç›®æ ‡"
    )
    scrape_parser.add_argument(
        "--append", action="store_true", default=False,
        help="è¿½åŠ æ¨¡å¼ï¼ˆä¸æ¸…é™¤é£ä¹¦å·²æœ‰æ•°æ®ï¼‰"
    )

    # categories å‘½ä»¤
    cat_parser = subparsers.add_parser("categories", help="åˆ—å‡ºå¯ç”¨åˆ†ç±»")
    cat_parser.add_argument(
        "--source", type=str, default=None,
        help="æ•°æ®æ¥æº (é»˜è®¤: fanqie)"
    )

    # feishu-fields å‘½ä»¤
    subparsers.add_parser("feishu-fields", help="æ˜¾ç¤ºé£ä¹¦è¡¨æ ¼æ‰€éœ€å­—æ®µ")

    # download å‘½ä»¤
    dl_parser = subparsers.add_parser("download", help="ä¸‹è½½å°è¯´")
    dl_parser.add_argument(
        "book_id", type=str,
        help="ä¹¦ç± ID (ä» fanqienovel.com/page/xxx ä¸­è·å–)"
    )
    dl_parser.add_argument(
        "--info-only", action="store_true",
        help="åªæ˜¾ç¤ºä¹¦ç±ä¿¡æ¯ï¼Œä¸ä¸‹è½½"
    )
    dl_parser.add_argument(
        "--chapters-only", action="store_true",
        help="åªæ˜¾ç¤ºç« èŠ‚åˆ—è¡¨ï¼Œä¸ä¸‹è½½"
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(0)

    config = load_config()

    if args.command == "scrape":
        cmd_scrape(args, config)
    elif args.command == "categories":
        cmd_categories(args, config)
    elif args.command == "feishu-fields":
        cmd_feishu_fields(args, config)
    elif args.command == "download":
        cmd_download(args, config)


if __name__ == "__main__":
    main()
